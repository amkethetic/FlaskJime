{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "indeed_scrapper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmrCb18I5Uu2"
      },
      "source": [
        "# Indeed Job Scraper\n",
        "create a general purpose job scraper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdgNa0LQ5UvC"
      },
      "source": [
        "import csv\n",
        "from datetime import datetime\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCfplUt_5Uvk"
      },
      "source": [
        "### getting jobs from indeed with informations with just giving a job title "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WzBENPE5Uvl"
      },
      "source": [
        "import csv\n",
        "from datetime import datetime\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def get_url(position, location):\n",
        "    \"\"\"Generate url from position and location\"\"\"\n",
        "    template = 'https://fr.indeed.com/jobs?q={}&l={}'\n",
        "    position = position.replace(' ', '+')\n",
        "    location = location.replace(' ', '+')\n",
        "    url = template.format(position, location)\n",
        "    return url\n",
        "\n",
        "\n",
        "def get_record(card):\n",
        "    \"\"\"Extract job data from a single record\"\"\"\n",
        "    \n",
        "    job_title = card.h2.span.get('title')\n",
        "    company = card.find('span', 'companyName').text\n",
        "    job_location = card.find('div', 'companyLocation').text\n",
        "    post_date = card.find('span', 'date').text\n",
        "    today = datetime.today().strftime('%Y-%m-%d')\n",
        "    summary = card.find('div', 'job-snippet').find_all('li')\n",
        "    summary = summary[0].get_text()\n",
        "    #job_url = 'https://www.indeed.com' + card.h2.a.get('href')\n",
        "\n",
        "    # this does not exists for all jobs, so handle the exceptions\n",
        "    salary_tag = card.find('span', 'salaryText')\n",
        "    if salary_tag:\n",
        "        salary = salary_tag.text.strip()\n",
        "    else:\n",
        "        salary = ''  \n",
        "        \n",
        "    record = (job_title, company, job_location, post_date, today, summary, salary)#, job_url)\n",
        "    return record\n",
        "\n",
        "\n",
        "def main(position, location):\n",
        "    \"\"\"Run the main program routine\"\"\"\n",
        "    records = []\n",
        "    url = get_url(position, location)\n",
        "    \n",
        "    # extract the job data\n",
        "    while True:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        cards = soup.find_all('div', 'job_seen_beacon')\n",
        "        for card in cards:\n",
        "            record = get_record(card)\n",
        "            records.append(record)\n",
        "        try:\n",
        "            url = 'https://www.indeed.com' + soup.find('a', {'aria-label': 'Next'}).get('href')\n",
        "        except AttributeError:\n",
        "            break\n",
        "    print(records)\n",
        "        \n",
        "    # save the job data\n",
        "    with open('results.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['JobTitle', 'Company', 'Location', 'PostDate', 'ExtractDate', 'Summary', 'Salary'])\n",
        "        writer.writerows(records)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY6Gzjn35Uvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaaa88d8-514f-40fb-fc46-63b5221a301d"
      },
      "source": [
        "# run the main program\n",
        "main('data engineer', 'paris')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(None, 'Siderlog', 'Paris (75)', 'il y a 7 jours', '2021-11-03', 'Vous travaillez en relation directe avec le client et intégrez une équipe de consultants Siderlog.', ''), ('Data Engineer Junior', 'NonStop Consulting', 'Clichy (92)', 'il y a 12 jours', '2021-11-03', \"L'entreprise en question est une start-up en plein développement qui recherche Data Engineer Junior en Freelance ou en CDI à CLICHY (92) en région parisienne.\", ''), (None, 'Capgemini Invent', 'Paris (75)', 'il y a 3 jours', '2021-11-03', 'Sectoriel Data Strategy est une entité multidisciplinaire regroupant consultants digitaux, data scientists, data engineers ainsi que des profils technologiques …', ''), (None, 'Alstom', 'Saint-Ouen (93)', 'il y a 1 jour', '2021-11-03', 'Take in charge the correction of the data and their segmentation according to the protocols as defined.', ''), (None, 'Société Générale', 'La Défense (92)+ 1\\xa0lieu', 'il y a 8 jours', '2021-11-03', 'Le Département Digital Transformation Office de la Direction des Risques du Groupe Société Générale recrute des collaborateurs pour renforcer son équipe en…', ''), (None, 'OCTO Technology', 'Paris (75)', 'il y a 7 jours', '2021-11-03', 'Formaliser des architectures data performantes et sécurisées sur GCP ou hybrides.', ''), ('Stage - Data Engineer H/F', 'BforBank', 'La Défense (92)', 'il y a 12 jours', '2021-11-03', 'Réaliser des analyses et des études d’impacts en amont des développements,.', ''), (None, 'Capgemini Invent', 'Paris (75)', 'il y a 3 jours', '2021-11-03', 'Nous lançons de nouveaux business models en partenariat avec les incubateurs / accélérateurs, les startups, les éditeurs technologiques ainsi que les clients…', ''), ('Data Engineer', 'Sendinblue', 'Paris (75)', 'il y a 9 jours', '2021-11-03', 'Improve the data quality through automated data quality monitoring.', ''), ('STAGIAIRE DATA ENGINEER', 'Lobellia', 'Montrouge (92)+ 1\\xa0lieu', 'il y a 13 jours', '2021-11-03', 'Vous interviendrez au sein d’une équipe constituée de 1 à 2 consultants expérimentés et d’un chef de projet.', ''), (None, 'Lydia Solutions', 'Paris 4e (75)•Télétravail temporaire', \"Aujourd'hui\", '2021-11-03', 'Build and maintain a scalable data workflow to collect data from events (Stream Processing, Eventing), prod databases and external APIs using state of the art…', ''), ('Junior Data Engineer', 'ORTEC', 'Paris (75)', 'il y a 30+ jours', '2021-11-03', 'In close collaboration with the client’s project leads, technical leads and engineers, the ORTEC Data Engineering team build the data ingestion and processing…', ''), ('Data Engineer', 'Doctrine', 'Paris (75)', 'il y a 30+ jours', '2021-11-03', 'Knowledge of data acquisition and modeling.', ''), (None, 'Extia', 'Paris (75)', 'il y a 3 jours', '2021-11-03', 'Vous travaillez en étroite relation avec les autres data engineers pour développer des applications assurant une bonne circulation des données (batch et stream)…', ''), (None, 'Positive Thinking Company', 'Paris (75)', 'il y a 7 jours', '2021-11-03', 'Vous avez au moins une expérience professionnelle significative en tant que data engineer dans le domaine du retail, idéalement.', '')]\n"
          ]
        }
      ]
    }
  ]
}